{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\tamojit\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.tree.tree module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.tree. Anything that cannot be imported from sklearn.tree is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "c:\\users\\tamojit\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.ensemble.weight_boosting module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "c:\\users\\tamojit\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.ensemble.forest module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "c:\\users\\tamojit\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.svm.classes module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.svm. Anything that cannot be imported from sklearn.svm is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "c:\\users\\tamojit\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.neighbors.classification module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn_porter\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22.082104</td>\n",
       "      <td>39.144303</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.670834</td>\n",
       "      <td>15.991502</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17.590614</td>\n",
       "      <td>10.987550</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.228235</td>\n",
       "      <td>23.188109</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25.695574</td>\n",
       "      <td>-28.947627</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          X1         X2  Y\n",
       "0  22.082104  39.144303  1\n",
       "1  13.670834  15.991502  1\n",
       "2  17.590614  10.987550  1\n",
       "3   6.228235  23.188109  1\n",
       "4  25.695574 -28.947627  4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=data.pop('Y')\n",
    "X=data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(X,Y,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=MLPClassifier(verbose=True,hidden_layer_sizes=[32,32],max_iter=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.71689441\n",
      "Iteration 2, loss = 1.52728142\n",
      "Iteration 3, loss = 1.35657465\n",
      "Iteration 4, loss = 1.20419471\n",
      "Iteration 5, loss = 1.06878793\n",
      "Iteration 6, loss = 0.94896183\n",
      "Iteration 7, loss = 0.84297818\n",
      "Iteration 8, loss = 0.74983573\n",
      "Iteration 9, loss = 0.66864903\n",
      "Iteration 10, loss = 0.59741027\n",
      "Iteration 11, loss = 0.53545586\n",
      "Iteration 12, loss = 0.48181570\n",
      "Iteration 13, loss = 0.43535517\n",
      "Iteration 14, loss = 0.39503471\n",
      "Iteration 15, loss = 0.35994078\n",
      "Iteration 16, loss = 0.32953737\n",
      "Iteration 17, loss = 0.30314138\n",
      "Iteration 18, loss = 0.28051053\n",
      "Iteration 19, loss = 0.26102709\n",
      "Iteration 20, loss = 0.24445561\n",
      "Iteration 21, loss = 0.23042501\n",
      "Iteration 22, loss = 0.21846312\n",
      "Iteration 23, loss = 0.20823336\n",
      "Iteration 24, loss = 0.19951490\n",
      "Iteration 25, loss = 0.19209553\n",
      "Iteration 26, loss = 0.18570262\n",
      "Iteration 27, loss = 0.18001975\n",
      "Iteration 28, loss = 0.17486561\n",
      "Iteration 29, loss = 0.17009798\n",
      "Iteration 30, loss = 0.16561032\n",
      "Iteration 31, loss = 0.16124711\n",
      "Iteration 32, loss = 0.15695089\n",
      "Iteration 33, loss = 0.15279071\n",
      "Iteration 34, loss = 0.14881376\n",
      "Iteration 35, loss = 0.14501618\n",
      "Iteration 36, loss = 0.14141935\n",
      "Iteration 37, loss = 0.13807844\n",
      "Iteration 38, loss = 0.13506729\n",
      "Iteration 39, loss = 0.13230456\n",
      "Iteration 40, loss = 0.12976915\n",
      "Iteration 41, loss = 0.12741918\n",
      "Iteration 42, loss = 0.12520719\n",
      "Iteration 43, loss = 0.12305532\n",
      "Iteration 44, loss = 0.12118775\n",
      "Iteration 45, loss = 0.11940958\n",
      "Iteration 46, loss = 0.11774739\n",
      "Iteration 47, loss = 0.11617573\n",
      "Iteration 48, loss = 0.11464062\n",
      "Iteration 49, loss = 0.11313478\n",
      "Iteration 50, loss = 0.11170485\n",
      "Iteration 51, loss = 0.11034418\n",
      "Iteration 52, loss = 0.10904044\n",
      "Iteration 53, loss = 0.10781100\n",
      "Iteration 54, loss = 0.10663867\n",
      "Iteration 55, loss = 0.10553309\n",
      "Iteration 56, loss = 0.10447293\n",
      "Iteration 57, loss = 0.10343068\n",
      "Iteration 58, loss = 0.10239696\n",
      "Iteration 59, loss = 0.10140226\n",
      "Iteration 60, loss = 0.10042547\n",
      "Iteration 61, loss = 0.09947052\n",
      "Iteration 62, loss = 0.09853898\n",
      "Iteration 63, loss = 0.09764543\n",
      "Iteration 64, loss = 0.09676884\n",
      "Iteration 65, loss = 0.09591417\n",
      "Iteration 66, loss = 0.09507778\n",
      "Iteration 67, loss = 0.09424577\n",
      "Iteration 68, loss = 0.09344973\n",
      "Iteration 69, loss = 0.09268052\n",
      "Iteration 70, loss = 0.09192391\n",
      "Iteration 71, loss = 0.09118160\n",
      "Iteration 72, loss = 0.09049390\n",
      "Iteration 73, loss = 0.08982372\n",
      "Iteration 74, loss = 0.08917108\n",
      "Iteration 75, loss = 0.08852709\n",
      "Iteration 76, loss = 0.08788681\n",
      "Iteration 77, loss = 0.08725753\n",
      "Iteration 78, loss = 0.08663788\n",
      "Iteration 79, loss = 0.08602807\n",
      "Iteration 80, loss = 0.08542647\n",
      "Iteration 81, loss = 0.08483343\n",
      "Iteration 82, loss = 0.08425082\n",
      "Iteration 83, loss = 0.08367552\n",
      "Iteration 84, loss = 0.08310582\n",
      "Iteration 85, loss = 0.08254470\n",
      "Iteration 86, loss = 0.08197941\n",
      "Iteration 87, loss = 0.08142413\n",
      "Iteration 88, loss = 0.08085718\n",
      "Iteration 89, loss = 0.08026603\n",
      "Iteration 90, loss = 0.07967346\n",
      "Iteration 91, loss = 0.07907397\n",
      "Iteration 92, loss = 0.07847399\n",
      "Iteration 93, loss = 0.07783826\n",
      "Iteration 94, loss = 0.07717753\n",
      "Iteration 95, loss = 0.07651564\n",
      "Iteration 96, loss = 0.07577926\n",
      "Iteration 97, loss = 0.07505143\n",
      "Iteration 98, loss = 0.07434849\n",
      "Iteration 99, loss = 0.07385130\n",
      "Iteration 100, loss = 0.07341672\n",
      "Iteration 101, loss = 0.07296000\n",
      "Iteration 102, loss = 0.07247115\n",
      "Iteration 103, loss = 0.07195409\n",
      "Iteration 104, loss = 0.07141531\n",
      "Iteration 105, loss = 0.07085774\n",
      "Iteration 106, loss = 0.07029392\n",
      "Iteration 107, loss = 0.06973506\n",
      "Iteration 108, loss = 0.06919414\n",
      "Iteration 109, loss = 0.06867806\n",
      "Iteration 110, loss = 0.06817802\n",
      "Iteration 111, loss = 0.06769010\n",
      "Iteration 112, loss = 0.06723198\n",
      "Iteration 113, loss = 0.06678740\n",
      "Iteration 114, loss = 0.06633417\n",
      "Iteration 115, loss = 0.06587273\n",
      "Iteration 116, loss = 0.06540762\n",
      "Iteration 117, loss = 0.06494624\n",
      "Iteration 118, loss = 0.06449668\n",
      "Iteration 119, loss = 0.06405486\n",
      "Iteration 120, loss = 0.06362358\n",
      "Iteration 121, loss = 0.06319709\n",
      "Iteration 122, loss = 0.06277525\n",
      "Iteration 123, loss = 0.06235887\n",
      "Iteration 124, loss = 0.06194600\n",
      "Iteration 125, loss = 0.06153652\n",
      "Iteration 126, loss = 0.06113155\n",
      "Iteration 127, loss = 0.06072935\n",
      "Iteration 128, loss = 0.06033272\n",
      "Iteration 129, loss = 0.05993995\n",
      "Iteration 130, loss = 0.05955146\n",
      "Iteration 131, loss = 0.05916563\n",
      "Iteration 132, loss = 0.05878299\n",
      "Iteration 133, loss = 0.05840546\n",
      "Iteration 134, loss = 0.05803027\n",
      "Iteration 135, loss = 0.05765658\n",
      "Iteration 136, loss = 0.05728650\n",
      "Iteration 137, loss = 0.05692249\n",
      "Iteration 138, loss = 0.05656057\n",
      "Iteration 139, loss = 0.05620288\n",
      "Iteration 140, loss = 0.05584805\n",
      "Iteration 141, loss = 0.05549598\n",
      "Iteration 142, loss = 0.05514717\n",
      "Iteration 143, loss = 0.05480040\n",
      "Iteration 144, loss = 0.05445763\n",
      "Iteration 145, loss = 0.05411715\n",
      "Iteration 146, loss = 0.05377876\n",
      "Iteration 147, loss = 0.05344390\n",
      "Iteration 148, loss = 0.05311243\n",
      "Iteration 149, loss = 0.05278273\n",
      "Iteration 150, loss = 0.05245658\n",
      "Iteration 151, loss = 0.05213212\n",
      "Iteration 152, loss = 0.05180897\n",
      "Iteration 153, loss = 0.05149090\n",
      "Iteration 154, loss = 0.05117459\n",
      "Iteration 155, loss = 0.05086033\n",
      "Iteration 156, loss = 0.05054216\n",
      "Iteration 157, loss = 0.05022650\n",
      "Iteration 158, loss = 0.04991326\n",
      "Iteration 159, loss = 0.04960044\n",
      "Iteration 160, loss = 0.04928503\n",
      "Iteration 161, loss = 0.04897325\n",
      "Iteration 162, loss = 0.04866200\n",
      "Iteration 163, loss = 0.04835086\n",
      "Iteration 164, loss = 0.04804015\n",
      "Iteration 165, loss = 0.04773263\n",
      "Iteration 166, loss = 0.04742728\n",
      "Iteration 167, loss = 0.04712175\n",
      "Iteration 168, loss = 0.04681673\n",
      "Iteration 169, loss = 0.04651478\n",
      "Iteration 170, loss = 0.04621542\n",
      "Iteration 171, loss = 0.04591657\n",
      "Iteration 172, loss = 0.04561895\n",
      "Iteration 173, loss = 0.04534269\n",
      "Iteration 174, loss = 0.04507036\n",
      "Iteration 175, loss = 0.04479950\n",
      "Iteration 176, loss = 0.04453037\n",
      "Iteration 177, loss = 0.04426329\n",
      "Iteration 178, loss = 0.04399969\n",
      "Iteration 179, loss = 0.04373926\n",
      "Iteration 180, loss = 0.04347962\n",
      "Iteration 181, loss = 0.04321889\n",
      "Iteration 182, loss = 0.04295931\n",
      "Iteration 183, loss = 0.04270111\n",
      "Iteration 184, loss = 0.04244420\n",
      "Iteration 185, loss = 0.04218864\n",
      "Iteration 186, loss = 0.04193376\n",
      "Iteration 187, loss = 0.04167348\n",
      "Iteration 188, loss = 0.04141282\n",
      "Iteration 189, loss = 0.04115277\n",
      "Iteration 190, loss = 0.04089358\n",
      "Iteration 191, loss = 0.04063518\n",
      "Iteration 192, loss = 0.04036765\n",
      "Iteration 193, loss = 0.04010407\n",
      "Iteration 194, loss = 0.03984549\n",
      "Iteration 195, loss = 0.03958684\n",
      "Iteration 196, loss = 0.03933513\n",
      "Iteration 197, loss = 0.03909932\n",
      "Iteration 198, loss = 0.03886569\n",
      "Iteration 199, loss = 0.03863390\n",
      "Iteration 200, loss = 0.03840403\n",
      "Iteration 201, loss = 0.03817603\n",
      "Iteration 202, loss = 0.03795005\n",
      "Iteration 203, loss = 0.03772618\n",
      "Iteration 204, loss = 0.03750437\n",
      "Iteration 205, loss = 0.03728497\n",
      "Iteration 206, loss = 0.03707285\n",
      "Iteration 207, loss = 0.03686241\n",
      "Iteration 208, loss = 0.03665488\n",
      "Iteration 209, loss = 0.03644947\n",
      "Iteration 210, loss = 0.03624530\n",
      "Iteration 211, loss = 0.03604315\n",
      "Iteration 212, loss = 0.03584298\n",
      "Iteration 213, loss = 0.03564509\n",
      "Iteration 214, loss = 0.03544858\n",
      "Iteration 215, loss = 0.03525398\n",
      "Iteration 216, loss = 0.03506051\n",
      "Iteration 217, loss = 0.03486877\n",
      "Iteration 218, loss = 0.03467840\n",
      "Iteration 219, loss = 0.03448958\n",
      "Iteration 220, loss = 0.03430219\n",
      "Iteration 221, loss = 0.03411636\n",
      "Iteration 222, loss = 0.03393171\n",
      "Iteration 223, loss = 0.03374957\n",
      "Iteration 224, loss = 0.03356766\n",
      "Iteration 225, loss = 0.03338739\n",
      "Iteration 226, loss = 0.03320872\n",
      "Iteration 227, loss = 0.03303181\n",
      "Iteration 228, loss = 0.03285608\n",
      "Iteration 229, loss = 0.03268156\n",
      "Iteration 230, loss = 0.03250835\n",
      "Iteration 231, loss = 0.03233619\n",
      "Iteration 232, loss = 0.03216534\n",
      "Iteration 233, loss = 0.03199564\n",
      "Iteration 234, loss = 0.03182861\n",
      "Iteration 235, loss = 0.03166069\n",
      "Iteration 236, loss = 0.03149647\n",
      "Iteration 237, loss = 0.03133349\n",
      "Iteration 238, loss = 0.03117157\n",
      "Iteration 239, loss = 0.03101142\n",
      "Iteration 240, loss = 0.03085112\n",
      "Iteration 241, loss = 0.03069347\n",
      "Iteration 242, loss = 0.03053564\n",
      "Iteration 243, loss = 0.03037931\n",
      "Iteration 244, loss = 0.03022435\n",
      "Iteration 245, loss = 0.03007019\n",
      "Iteration 246, loss = 0.02991732\n",
      "Iteration 247, loss = 0.02976489\n",
      "Iteration 248, loss = 0.02961333\n",
      "Iteration 249, loss = 0.02946324\n",
      "Iteration 250, loss = 0.02931327\n",
      "Iteration 251, loss = 0.02916461\n",
      "Iteration 252, loss = 0.02901683\n",
      "Iteration 253, loss = 0.02886980\n",
      "Iteration 254, loss = 0.02872361\n",
      "Iteration 255, loss = 0.02857940\n",
      "Iteration 256, loss = 0.02843464\n",
      "Iteration 257, loss = 0.02829096\n",
      "Iteration 258, loss = 0.02814866\n",
      "Iteration 259, loss = 0.02800691\n",
      "Iteration 260, loss = 0.02786625\n",
      "Iteration 261, loss = 0.02772610\n",
      "Iteration 262, loss = 0.02758663\n",
      "Iteration 263, loss = 0.02744870\n",
      "Iteration 264, loss = 0.02731054\n",
      "Iteration 265, loss = 0.02717387\n",
      "Iteration 266, loss = 0.02703785\n",
      "Iteration 267, loss = 0.02690282\n",
      "Iteration 268, loss = 0.02676811\n",
      "Iteration 269, loss = 0.02663467\n",
      "Iteration 270, loss = 0.02650141\n",
      "Iteration 271, loss = 0.02636926\n",
      "Iteration 272, loss = 0.02623876\n",
      "Iteration 273, loss = 0.02610923\n",
      "Iteration 274, loss = 0.02598110\n",
      "Iteration 275, loss = 0.02585142\n",
      "Iteration 276, loss = 0.02572524\n",
      "Iteration 277, loss = 0.02559766\n",
      "Iteration 278, loss = 0.02547078\n",
      "Iteration 279, loss = 0.02534506\n",
      "Iteration 280, loss = 0.02521994\n",
      "Iteration 281, loss = 0.02509512\n",
      "Iteration 282, loss = 0.02497106\n",
      "Iteration 283, loss = 0.02484746\n",
      "Iteration 284, loss = 0.02472458\n",
      "Iteration 285, loss = 0.02460217\n",
      "Iteration 286, loss = 0.02448045\n",
      "Iteration 287, loss = 0.02435942\n",
      "Iteration 288, loss = 0.02423894\n",
      "Iteration 289, loss = 0.02411923\n",
      "Iteration 290, loss = 0.02400032\n",
      "Iteration 291, loss = 0.02388231\n",
      "Iteration 292, loss = 0.02376460\n",
      "Iteration 293, loss = 0.02364770\n",
      "Iteration 294, loss = 0.02353193\n",
      "Iteration 295, loss = 0.02341710\n",
      "Iteration 296, loss = 0.02330274\n",
      "Iteration 297, loss = 0.02318275\n",
      "Iteration 298, loss = 0.02306112\n",
      "Iteration 299, loss = 0.02293929\n",
      "Iteration 300, loss = 0.02281868\n",
      "Iteration 301, loss = 0.02269468\n",
      "Iteration 302, loss = 0.02257981\n",
      "Iteration 303, loss = 0.02246589\n",
      "Iteration 304, loss = 0.02235476\n",
      "Iteration 305, loss = 0.02224310\n",
      "Iteration 306, loss = 0.02213048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 307, loss = 0.02201655\n",
      "Iteration 308, loss = 0.02190280\n",
      "Iteration 309, loss = 0.02178746\n",
      "Iteration 310, loss = 0.02167006\n",
      "Iteration 311, loss = 0.02155359\n",
      "Iteration 312, loss = 0.02144292\n",
      "Iteration 313, loss = 0.02133116\n",
      "Iteration 314, loss = 0.02122058\n",
      "Iteration 315, loss = 0.02110914\n",
      "Iteration 316, loss = 0.02099466\n",
      "Iteration 317, loss = 0.02088175\n",
      "Iteration 318, loss = 0.02077243\n",
      "Iteration 319, loss = 0.02066541\n",
      "Iteration 320, loss = 0.02055657\n",
      "Iteration 321, loss = 0.02044655\n",
      "Iteration 322, loss = 0.02033580\n",
      "Iteration 323, loss = 0.02022327\n",
      "Iteration 324, loss = 0.02011346\n",
      "Iteration 325, loss = 0.02000235\n",
      "Iteration 326, loss = 0.01989414\n",
      "Iteration 327, loss = 0.01978441\n",
      "Iteration 328, loss = 0.01967413\n",
      "Iteration 329, loss = 0.01956629\n",
      "Iteration 330, loss = 0.01946014\n",
      "Iteration 331, loss = 0.01935527\n",
      "Iteration 332, loss = 0.01924847\n",
      "Iteration 333, loss = 0.01913957\n",
      "Iteration 334, loss = 0.01902963\n",
      "Iteration 335, loss = 0.01893190\n",
      "Iteration 336, loss = 0.01882240\n",
      "Iteration 337, loss = 0.01871105\n",
      "Iteration 338, loss = 0.01859097\n",
      "Iteration 339, loss = 0.01847022\n",
      "Iteration 340, loss = 0.01834956\n",
      "Iteration 341, loss = 0.01822890\n",
      "Iteration 342, loss = 0.01810800\n",
      "Iteration 343, loss = 0.01798683\n",
      "Iteration 344, loss = 0.01787573\n",
      "Iteration 345, loss = 0.01776770\n",
      "Iteration 346, loss = 0.01766152\n",
      "Iteration 347, loss = 0.01755564\n",
      "Iteration 348, loss = 0.01744931\n",
      "Iteration 349, loss = 0.01734362\n",
      "Iteration 350, loss = 0.01723936\n",
      "Iteration 351, loss = 0.01713609\n",
      "Iteration 352, loss = 0.01703362\n",
      "Iteration 353, loss = 0.01693167\n",
      "Iteration 354, loss = 0.01682876\n",
      "Iteration 355, loss = 0.01672578\n",
      "Iteration 356, loss = 0.01662245\n",
      "Iteration 357, loss = 0.01651947\n",
      "Iteration 358, loss = 0.01641716\n",
      "Iteration 359, loss = 0.01631577\n",
      "Iteration 360, loss = 0.01621612\n",
      "Iteration 361, loss = 0.01611980\n",
      "Iteration 362, loss = 0.01602126\n",
      "Iteration 363, loss = 0.01592163\n",
      "Iteration 364, loss = 0.01582493\n",
      "Iteration 365, loss = 0.01572838\n",
      "Iteration 366, loss = 0.01563245\n",
      "Iteration 367, loss = 0.01553684\n",
      "Iteration 368, loss = 0.01544179\n",
      "Iteration 369, loss = 0.01534722\n",
      "Iteration 370, loss = 0.01525294\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=[32, 32], learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_fun=15000, max_iter=800,\n",
       "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "              tol=0.0001, validation_fraction=0.1, verbose=True,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(model.loss_curve_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([[-7.8,-4.1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([[-4.1,4.3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([[-7.8,-40.2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([[7.8,-4.1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class MLPClassifier {\n",
      "\n",
      "    private enum Activation { IDENTITY, LOGISTIC, RELU, TANH, SOFTMAX }\n",
      "\n",
      "    private Activation hidden;\n",
      "    private Activation output;\n",
      "    private double[][] network;\n",
      "    private double[][][] weights;\n",
      "    private double[][] bias;\n",
      "\n",
      "    public MLPClassifier(String hidden, String output, int[] layers, double[][][] weights, double[][] bias) {\n",
      "        this.hidden = Activation.valueOf(hidden.toUpperCase());\n",
      "        this.output = Activation.valueOf(output.toUpperCase());\n",
      "        this.network = new double[layers.length + 1][];\n",
      "        for (int i = 0, l = layers.length; i < l; i++) {\n",
      "            this.network[i + 1] = new double[layers[i]];\n",
      "        }\n",
      "        this.weights = weights;\n",
      "        this.bias = bias;\n",
      "    }\n",
      "\n",
      "    public MLPClassifier(String hidden, String output, int neurons, double[][][] weights, double[][] bias) {\n",
      "        this(hidden, output, new int[] { neurons }, weights, bias);\n",
      "    }\n",
      "\n",
      "    private double[] compute(Activation activation, double[] v) {\n",
      "        switch (activation) {\n",
      "            case LOGISTIC:\n",
      "                for (int i = 0, l = v.length; i < l; i++) {\n",
      "                    v[i] = 1. / (1. + Math.exp(-v[i]));\n",
      "                }\n",
      "                break;\n",
      "            case RELU:\n",
      "                for (int i = 0, l = v.length; i < l; i++) {\n",
      "                    v[i] = Math.max(0, v[i]);\n",
      "                }\n",
      "                break;\n",
      "            case TANH:\n",
      "                for (int i = 0, l = v.length; i < l; i++) {\n",
      "                    v[i] = Math.tanh(v[i]);\n",
      "                }\n",
      "                break;\n",
      "            case SOFTMAX:\n",
      "                double max = Double.NEGATIVE_INFINITY;\n",
      "                for (double x : v) {\n",
      "                    if (x > max) {\n",
      "                        max = x;\n",
      "                    }\n",
      "                }\n",
      "                for (int i = 0, l = v.length; i < l; i++) {\n",
      "                    v[i] = Math.exp(v[i] - max);\n",
      "                }\n",
      "                double sum = 0.;\n",
      "                for (double x : v) {\n",
      "                    sum += x;\n",
      "                }\n",
      "                for (int i = 0, l = v.length; i < l; i++) {\n",
      "                    v[i] /= sum;\n",
      "                }\n",
      "                break;\n",
      "        }\n",
      "        return v;\n",
      "    }\n",
      "\n",
      "    public int predict(double[] neurons) {\n",
      "        this.network[0] = neurons;\n",
      "\n",
      "        for (int i = 0; i < this.network.length - 1; i++) {\n",
      "            for (int j = 0; j < this.network[i + 1].length; j++) {\n",
      "                this.network[i + 1][j] = this.bias[i][j];\n",
      "                for (int l = 0; l < this.network[i].length; l++) {\n",
      "                    this.network[i + 1][j] += this.network[i][l] * this.weights[i][l][j];\n",
      "                }\n",
      "            }\n",
      "            if ((i + 1) < (this.network.length - 1)) {\n",
      "                this.network[i + 1] = this.compute(this.hidden, this.network[i + 1]);\n",
      "            }\n",
      "        }\n",
      "        this.network[this.network.length - 1] = this.compute(this.output, this.network[this.network.length - 1]);\n",
      "\n",
      "        if (this.network[this.network.length - 1].length == 1) {\n",
      "            if (this.network[this.network.length - 1][0] > .5) {\n",
      "                return 1;\n",
      "            }\n",
      "            return 0;\n",
      "        } else {\n",
      "            int classIdx = 0;\n",
      "            for (int i = 0; i < this.network[this.network.length - 1].length; i++) {\n",
      "                classIdx = this.network[this.network.length - 1][i] > this.network[this.network.length - 1][classIdx] ? i : classIdx;\n",
      "            }\n",
      "            return classIdx;\n",
      "        }\n",
      "\n",
      "    }\n",
      "\n",
      "    public static void main(String[] args) {\n",
      "        if (args.length == 2) {\n",
      "\n",
      "            // Features:\n",
      "            double[] features = new double[args.length];\n",
      "            for (int i = 0, l = args.length; i < l; i++) {\n",
      "                features[i] = Double.parseDouble(args[i]);\n",
      "            }\n",
      "\n",
      "            // Parameters:\n",
      "            int[] layers = {32, 32, 4};\n",
      "            double[][][] weights = {{{-0.35233439837787506, 0.0877935459505192, -0.08042235270346546, -0.31347670243913306, 0.3403961434723505, -0.0880396174586327, -0.37927780806457434, 0.15791993006165064, 0.28365672043218876, -0.0872936638875485, 0.22723927561251703, 0.2937588434778232, 0.4227621411277713, 0.17107063644606965, -0.16153106596527872, -0.4286695962590158, -0.19648324292117042, -0.0014152562999172327, 0.08953529904976198, 0.13561745467654554, -0.006553274663851916, 0.36135508696207386, -0.20984493971165982, 0.23164240912917847, -0.23843412766895522, -0.009201025779600813, -0.07702087207821193, -0.27359314225607034, 0.14701321374759269, -0.29525080698778605, 0.4309227551796704, 0.17747623419412167}, {0.19751033935998671, 0.16230088412012889, 0.08554911741804981, -0.3672835138407148, 0.09488653207322874, -0.24676958513987213, 0.19518689022702668, 0.17546573493556736, -0.0745685596506006, 0.29093536801797526, 0.01296750849841521, 0.30220627829350666, 0.1681342365222011, 0.22690940700221326, 0.11913241086571869, -0.17191087565162244, 0.3869321140348034, 0.3860266012593748, 0.35313496604932954, -0.04835602020853289, -0.0013118720962692024, -0.3254076332254449, 0.24346304166484362, 0.40724042669193583, -0.09600520881390356, 0.1687850872626568, -0.2827441181107278, 0.30895771113060894, -0.3004418137986187, -0.3736420723221597, 0.29000860612699575, -0.2743289684101431}}, {{0.2257840018665079, -0.007839315688200524, -0.24107931321111498, 0.04314652471071833, -0.10791506547803066, -0.3698088390565387, -0.046146699734372375, -0.2390423525842177, -0.30334125991580346, -0.1897074566380424, 0.04370889717756421, -0.2556070454222397, -0.13217847410177727, 0.2732564455727665, -0.18455702577153374, -0.044343691015185635, -0.016748821201951637, 0.24412206552840154, -0.1492585833133265, -0.0844527065120066, -0.0400043828501388, -0.04017389188170899, -0.23258438041300805, 0.12406386440580838, -0.011764581850909691, -0.19046431838128153, -0.028962329755735997, -0.02589604592707603, -0.0128357995306825, 0.0002644622405795537, -0.1498499699928574, 0.3033433634254875}, {-0.1624616466705623, -0.08717881857550977, 0.2851067763539398, 0.11579332210874786, -0.2986293329022779, 0.04918749692054171, -0.00771703947110653, -0.11617567363005593, 0.21596711552023473, 0.12266566972813552, 0.2267168491699751, -0.12806207562989008, 0.03689726960297698, -1.4924997037128788e-08, 0.00343405022053267, 0.08167669580187722, -0.1926740561712957, 0.1708620481878576, -0.0068854830117589675, 0.04745877771086298, -0.004171227839874505, -0.0544507598582826, 0.14276897380831424, 0.16075476383430523, -0.033543629936516446, -0.31266675331661786, -0.07240898136105316, -0.05851199345014859, -0.34141776612111874, -0.016513616074906177, 0.10351590125808763, -0.32394603256669846}, {-0.26075303473232675, -0.146830043101457, -0.015955874622513376, 0.16703261313773585, -0.0178427131834851, -0.19147556722329603, 0.017566929577183085, 0.21036200200065677, -0.14941052506324817, -0.08178325945214308, -0.2457047286222742, -0.19322473706076404, -0.22113980649849158, -0.17446609060835364, 0.09931392833542434, -0.07103908293362783, -0.13707830320495182, 0.0328952053682135, 0.21861076871891683, -0.15859813354646426, 0.06041785653922423, -0.35192855861145383, 0.1408888896027867, 0.24921087509967615, -0.19952786039545195, -0.10241874778069635, -0.10580615543869532, 0.018029846104019778, -0.23377439126031435, 7.993953984802381e-07, 0.12777503872749071, -0.19960245756671716}, {-0.20688602357459634, -0.1638080201292832, -0.04837010222675579, -0.2169684059404106, -0.009114872828463067, -0.011671250627950908, 0.024425937723202215, 0.32310420428060016, 0.33128986780798647, 0.011791728232732702, 0.22791663530875939, 0.1737149194730057, 0.10995174750296094, 0.11684144840354661, 0.1734557164173782, -0.22431585224772493, 0.2709447052563439, -0.13692139071998852, 0.15387557700531268, -0.2273059247601915, 0.05138556560105915, 0.05273863057772241, 0.025997848024284294, -0.2593660753344173, 0.08298903024420147, 0.09805767418990284, 0.19441797670636332, -0.17576822818707025, 0.14632476891129675, -0.016619242231133785, -0.1279179873577923, 0.0924461106377805}, {0.34979802172012325, 0.23339950571111834, 0.2534722925975337, 0.09392403649226475, -0.20213711230120676, -0.03142786175034709, -0.0388374270951087, 0.03185140585895813, -0.2248390100159069, 0.040446624604683824, 0.2079689324382609, -0.10141023721849199, -0.22331096544771445, 0.0766595952895663, 0.2959917807728463, 0.3057192390538923, 0.15464289767713865, -0.2500776093525511, 0.06889646271394798, -0.23290651674266533, 0.051330651037518105, -0.021892237874128636, -0.27979277547397075, 0.10445750571686961, -0.22204414013321291, -0.1877429991479137, -0.5118780638617588, -0.10522114239520787, -0.04103503361708492, -0.02447598015525049, -0.30729296709693715, -3.5141377774290516e-05}, {0.021533462007417372, 0.18860892370164203, -0.051967149279386386, -0.023430977370565542, -0.26546146640287727, -0.00024964960486284173, -0.011035945415517494, 0.01326914939084229, -0.07808897833010711, 0.0008740647746225461, 0.010763452687331797, -0.16334725952705073, 0.21034468277577165, 0.20210794934683635, 0.1311151024691052, 0.1932706704389476, 0.2807397872626687, 0.025168452432320475, -0.21675653847903473, 0.3501287062965015, -0.00981299561637786, -0.23820626193443592, -0.07370355836558412, 0.33821034589014354, 0.28371105701235166, -0.23051409955449084, 0.16471951338828017, 0.31053593739257757, 0.26896513439153424, -0.0796628901545834, 0.14391061965461405, 0.07555825500110669}, {0.16184254222886868, 0.20848143185794027, 0.1573881891886654, 0.22814515579957265, 0.2600356600672864, -0.6149140309575603, -0.0014679166368097884, 0.23675990928785598, -0.07455925162863421, -0.19211277693639986, 0.24106501895532143, -0.19568790518249848, -0.07796481674929108, -0.2784604042575249, 0.229702044953935, -0.07915425827383085, -0.18510928306228105, 0.32531208210157986, 0.1300597704566856, 0.04434587616486633, -0.06161262313092539, -0.2158420514439221, 0.07068148023801986, -0.04815139195511605, 0.373063393941739, -0.12519757129170667, 0.06983846116701785, 0.1649676261966087, -0.2686049453871815, -0.06267611598289562, 0.3371186794776713, 0.29605479046209754}, {0.20813247046328778, -0.19717463734794954, 0.06250539933286113, 0.23520023330786283, 0.047666047223339565, 0.24052058900580325, -0.02086295953688241, 0.17016982388516588, 0.3008818008972463, -0.06087752826325142, -0.20445553192051136, -0.04471452247638905, 0.25481996118364986, -0.06561458138673137, 0.20467949391792964, 0.23018777615815877, -0.23731271123405473, 0.13637560521634295, 0.271840903023927, 0.061940031960244224, -1.1869529801651366e-06, -0.11752701712038452, -0.22640933672340668, 0.018479725138953625, -0.02871567257623617, -0.1091573215424276, -0.013256057918063908, 0.4452356836770953, -0.3346384969255839, -0.041916508860133275, -0.22843242108859665, 0.11384735501764215}, {0.15414291485949413, 0.19372408627647492, 0.10648272134974494, -0.27421172866773186, -0.186580741617399, -0.048220704920061905, 0.007929334989360598, -0.08064844469076239, -0.04169377531209382, -0.014755277974588515, 0.036384299524873175, 0.2393379514079911, 0.2769488836191025, 0.0030082742709924624, 0.254491315471162, 0.25047866047365985, 0.24313292904230377, -0.09784991551930879, -0.007643541777983127, -0.32739279314705433, -0.024858034466056604, 0.32335022678663505, 0.10400893243106303, -0.11458868077485582, 0.0401807013563723, -0.06218834292359897, -0.3915455295028731, -0.08423418673194166, 0.04458269741407033, -0.07078737705975215, 0.13737909011056929, 0.038090543335979624}, {-0.25399042721834986, -0.035277957798691986, -0.2532686724101401, 0.1682581637312228, 0.12068648912050413, 0.34718199368698177, 5.926983557600504e-05, 0.03695100363244871, 0.16326712385634315, -0.08365915892209501, 0.10971661705671927, -0.16917438792115133, -0.08440415525477005, 0.178110190446283, -0.1977554246404469, 0.24111827625551663, 0.045532242722019244, -0.04364743649877085, 0.25826816912825057, -0.030232537870007844, 3.838167465028762e-05, -0.29149726700808465, -0.1340563292134781, 0.1116223081696496, -0.12915705773942243, 0.14688975547638344, -0.4439412424668051, 0.10879392690960682, 0.04992161643944516, 5.954603259760954e-07, -0.23147444827515173, -0.2496349807415728}, {0.06591579426759403, 0.23398256993822591, -0.06907724163487697, 0.0695563898980195, 0.1009730697477385, -0.1825502254149732, -0.10403700754147756, -0.43994255081250067, -0.211159713260828, 0.0005940511534842491, 0.27780148595596355, 0.1299827695488323, -0.15843064360814466, 0.002746379170595059, 0.1610274850071527, -0.20307502546128672, -0.23518393709023175, 0.12235287998312332, 0.03471890015333353, 0.009066728186997712, -0.003815537837595847, 0.3070295106841943, -0.20958406479667172, 0.21485622210678385, 0.11091502311561172, 0.12698484718688008, -0.5501929696261051, 0.29759180912610106, 0.21170096489721146, -0.2143186665032044, -0.18958380521560786, -0.1476550421669536}, {-0.15445618471117306, -0.21749934539673943, -0.07791373245366742, -0.023145665858324366, 0.16287545223551966, 0.1602711864606796, -0.01261645922325552, -0.07348914573441254, -0.025945734324428236, 0.08279398482111971, 0.2703965254399954, 0.09813465385044999, -0.25554045317590435, -0.07117852567625368, -0.000426591145415942, 0.23983084578418948, -0.20282052732790268, -0.052788948668936325, 0.1837700845761291, 0.0017696443109480903, -0.025936805201488337, -0.14821628717512877, 0.13274369540404155, 0.18989381473984612, -0.0697033381299121, -0.25372602412782486, -8.459957599914892e-05, -0.15165325227219478, 0.15112393120104242, 1.9680135691288782e-08, 0.24593266139400727, -0.23954466093097648}, {0.3189266478775888, 0.16475539604242948, -0.07145402956550456, -0.11092832196617393, 0.1934709838631397, 0.2685482661998112, -0.06560951225003306, -0.0036548368370533145, 0.20380663839611252, 0.0593603230535734, 0.10846137403395852, 0.09015308740416113, 0.24552559268100224, 0.0035167462405803407, 0.16927038403871697, 0.06860585977425558, -0.1271077839666856, -0.12195002371828034, 0.015206785186868889, 4.657717058332529e-08, -0.05119868546756788, 0.18475618306497052, -0.00856317586082349, -0.03744498400252163, 0.06636428046266235, 0.2585068619034453, -0.01297143690955651, 0.1899923908149202, 0.08928184274300179, -0.0016198098046829334, 0.022462269400901083, 0.014513358489513625}, {-0.13818887237374958, 0.18964693267345914, -0.17499378234837953, -0.017230406759787303, 0.17228266671114764, 0.11756974390566095, -0.051137412811501776, 0.018199703532106527, -0.27051878610979296, 0.15543761447843316, 0.2347261110019104, 0.017500410122717816, 0.2016243685263642, -0.0050362515313156385, 0.17322122651484734, -0.1906803847620052, -0.0844919741763848, 0.022461248598185234, -0.009257710302662403, -0.0756742273841332, 0.033942014334968594, -0.1919318662640442, 0.2559741182099347, 0.062004384455354965, 0.19821847247261323, -0.08293819331298191, 0.0734617564301555, -0.04727035013766861, -0.3193288954075492, -0.0067774491204764375, -0.03837319180622142, -0.2836065761984447}, {0.2219980037255838, 0.026781444817445977, -0.0408966435408975, -0.1762385990139062, 0.17085866918034612, -0.6133697630161568, -0.0460005772029802, -0.25935173140096707, 0.08191142182076262, -0.12056361072446391, -0.25715357992097254, 0.021341108161426495, -0.009556636316890273, -0.1566615312649463, 0.024325760125537825, -0.10970708592426977, -0.19376239482369556, 0.2972949155733068, -0.16268350421291985, 0.29418326747023693, -0.036711794057825674, 0.1724453921390452, 0.2578996296258882, 0.19968960227019256, 0.27363150714623974, 0.10259639987942264, 0.09171739101068019, -0.2749251519492276, -0.11476545427580732, 0.00801992367167136, -0.05940134700074868, 0.031129788199622074}, {-0.0766200924388719, -0.06917397120707863, 0.2675452735073862, -0.16474422892078092, -0.19684801985808048, -0.14599751378475923, -1.9878405316246862e-09, -0.045366316555937855, -0.15982213724490424, -0.015062718865050171, 0.04558241879349951, 0.10770695177898731, 0.05434526740666396, -0.17418552158774575, -0.3414692981177906, 0.22279424487984184, 0.21347558859068652, 0.3593683869670443, -0.1352271447692745, 0.22162019507234257, -0.030105327177960012, -0.1465109058551463, 0.01369376618067334, 0.022233203889158504, 0.09220969141363465, -0.28455890508864795, 0.12765261533424593, -0.23786352597913155, 0.1512144050563013, -0.16063280638867983, 0.2915272333226099, 0.061456123581479546}, {-0.1637428269997589, 0.09052807236547783, -0.10437667656273972, 0.03574466092442108, 0.2600498918485867, -0.11198393320524347, -0.03208240896827898, -0.22694111020841787, -0.058491623857434005, -0.03770996433735487, -0.2445762583390509, -0.11009304932572432, 0.3031914757955817, 0.13803977585118382, -0.1330523414241377, 0.2515232535081474, 0.24629467332539873, 0.2128327028116634, 0.0863053308512331, -0.1183650795813574, 0.00014220365741606384, -0.09377435982755067, 0.14249146020870027, -0.007063256920682594, 0.32929586172143416, -0.08069199749780512, -0.251137066041518, -0.3163189466519266, 0.16092989082733314, -0.00015038917269093635, 0.13870372000991635, -0.1945103765402574}, {-0.06137161339226395, 0.0833799440196695, -0.06844041631722952, 0.010196764436156846, 0.24090678666828305, 0.5336052810771865, -0.019462281950417666, -0.23474589675793459, 0.06763578049909048, 0.0364157152051089, -0.018000675587331587, -0.021697923349808357, 0.11539527690085397, -0.18624901334553357, 0.05119625489959909, 0.2217184725241866, -0.6163966710448592, -0.06412018372716294, 0.12677910886194577, -0.37488164647658606, -0.016673691086321236, 0.1790248180100796, 0.2804974373214889, -0.009609193510152533, 0.244503629463543, -0.13690810228236722, -0.5114585487769632, -0.36178976778588634, -0.002764023468856848, 0.0399769704489537, 0.16794677891461685, -0.004883673282009933}, {0.003835934859245817, 0.18700412841056127, -0.02629144502558202, 0.2709836585144081, 0.06681506075633992, 0.469761590296618, -0.005327460247498508, 0.13798363899686655, -0.2122933226724863, 0.09965817934901959, 0.1961001871320506, 0.2080505244561025, -0.0376708526260555, 0.00012529517232577232, -0.1690383958325912, 0.16683041039761343, 0.06361029652380333, 0.07769137830550657, 0.028929924907886703, -0.1600281956570047, -0.0322320475957219, -0.10397818970937744, 0.24812332182692762, -0.33430969382771747, -0.08985390936827752, 0.07057455034485205, 1.6561138996392453e-08, -0.2284727294062929, 0.0634062307890776, 0.003881669956917063, -0.1831794759382904, 0.21005991729730505}, {0.3574386991773592, 0.010186310326951387, 0.02633504528904834, 0.14666188017168977, -0.2710075603257338, -0.33739376835849394, -0.014889442183389882, 0.008193608891448888, -0.2639808755560615, -0.15484451545854444, -0.08705877912045311, 0.019404383179248256, -0.1388587774096085, 0.007958555511962716, 0.08896885011841527, -0.2566023656407329, 0.26205728457170835, -0.13079627575220162, -0.3239026900557751, -0.24953089306788745, -0.042486542311312915, 0.3183529517290646, -0.18533649683466896, 0.3424091109991446, 0.33579320775157206, -0.028074762165193976, 0.12726710525698826, -0.21624767759199465, -0.21406551145165229, -0.25928331885834377, -0.051597591861517106, 0.19741177779365898}, {0.09056915048641459, -0.0162224445778154, 0.1398682038405117, -0.14561637736110097, 0.2615689669029405, 1.1397306850439378e-05, 0.0010091056935021812, -0.2516264838370707, -0.074663551916383, 9.946492096205312e-08, 0.27489011613117187, -0.05692122758078126, -0.3064648455992723, 0.1945031878860127, 0.2040584832931904, 0.1442778043073276, 0.298012022720363, -0.19781569499476923, -0.007177534573734508, 0.22251027750771032, -0.07662167603606533, -0.04635751579344407, 0.19611279598001077, 0.0768710107289949, -0.18994249573870461, -0.02843818097929425, 0.11032167103201258, -0.1623782736320439, -0.0011114365737725051, -0.09129782893556432, 0.14082105413464674, 0.004444625682367922}, {0.283293061245815, -0.311483802680891, -0.14560452479306393, -0.03062734867130442, -0.23052314604162136, -0.19052242778917586, -0.016922984606879583, -0.24550746589219777, -0.1768452585248465, -0.19423894315687942, -0.1742793080638459, -0.2444036773987991, 0.32953809484205016, -0.05849558392907089, 0.15994462243532634, 0.17481770898214438, -0.18262354576310272, -0.10294883007307548, 0.1484589008291619, -0.24773972322835305, 0.050120065497199134, -0.1284145987300027, 0.03961803478674478, -0.011002374775200233, 0.05769491370878402, 0.0960669505710779, -0.03460915887518454, 0.11109519986521861, -0.10610638914256117, -0.006498206069702246, 0.04531247321900355, 0.17827502865675593}, {-0.057228470459944385, -0.0843930705855982, -0.2984176265071495, 0.08304762612112704, -0.250838906216944, -0.12347148850105683, 0.002272317991472422, 0.014616732449498574, 0.11535004319085689, -0.03932279175810171, -0.25347684105884, -0.022910694993402925, 0.23151720215929464, -0.2509015550080119, -0.07243845202278822, -0.29012051244265147, -0.2541683766464721, -0.18670221398402376, -0.23553368078533993, -0.250059199597758, 0.04867873875732559, -0.07366095321739712, -0.1691835826061455, 0.040059264681961936, 0.1660304821984107, -0.06442953699007338, -0.2328584547417595, 0.2258739728004789, -0.32443541269251747, -0.009127015336801107, 0.22992367054105392, -0.06655153504288652}, {0.16802871668115937, 0.06624047507458715, -0.1880901314512042, 0.2036783285007754, 0.12515938978134852, 0.1861161882344613, 9.830539995837498e-06, -0.2648579534591088, 0.19710561165459467, 0.13688628128259112, 0.09280849925783512, -0.0059031576196012505, 0.28307735297674236, -0.005377779624962774, 0.049071631963236016, 0.007039466415496305, -0.06796526823806774, 0.08741432354244043, -0.11851499267671532, -0.27578188615353977, -0.024787477662323257, 0.1282447069258779, 0.29170359412151153, -0.04926745561750477, -0.25727248233664624, 0.26597788510839554, -2.240652919767277e-06, 0.35481840451232044, 0.015019497189934145, 0.00908904826231248, 0.2533033847316933, 0.261990787355392}, {-0.19031628121899571, -0.25947866315176904, 0.14399916863316928, 0.2857115516968271, -0.3024641681463546, -0.3325414043178489, -0.11295300220331483, 0.30614949969234867, 0.21866100850445458, -0.26833888750565643, 0.18255106482193845, 0.1725501514736188, -0.03439254428285131, 0.1222058039490724, -0.2908844935332167, -0.14195417291719437, 0.2813462270775601, 0.28297901643220796, -0.22900887752737475, -0.08403167466860832, 7.773292372617333e-08, -0.1543269537705299, 0.2524001299542757, 0.24865086132955372, -0.07008196296656366, -0.15105053545676358, 0.23303354975034063, -0.2262877593995818, -0.48741754689207384, -0.13446428911850575, 0.2777306482156938, -0.23381369885376496}, {-0.2332594354000167, 0.28184248475197404, -0.2672049867025237, 0.15973110798325518, -0.02592587497574619, 0.4392607095214463, -0.01006906131556114, -0.2610785439910203, 0.0006208270792255492, 0.011247230299192031, 0.07778677851842664, -0.14738190570600804, -0.1202709997078599, -0.07467089673177453, -0.1710791488577525, -0.2598021453119132, -0.1772609134712759, -0.12268700728474943, 0.18135581055086675, -0.19980669749253235, 0.016627057429248075, 0.180689789610292, -0.2328218652226153, 0.05868887046030289, 0.09597015864950686, 0.1341045373638563, -0.18796745680181423, 0.032119071693645536, 0.12802637670164646, -3.800891148521348e-09, 0.25094766293661175, 0.14441771898969433}, {-0.22942227984068456, 0.14851080361564176, -0.04077098657075456, 0.061638291680039876, 0.27633645732636736, -0.5340257931403765, -0.07082229776190364, -0.04371669665536548, 0.18965299565910598, -0.1724577367789583, 0.06780751309878774, 0.0256611597990984, 0.3275010070823225, 0.01247721936264406, -0.11931784099581193, -0.1359831416827446, 0.20219975387584466, 0.19398435069067052, 0.05429128245309121, 0.2781535138084176, -0.06858270090763245, 0.30161971022572787, 0.22089034622902137, 0.276418406586554, 0.1389928456111703, 0.19559825072683834, 0.2661100879475268, 0.2029312415463688, -0.2548639287472383, -0.24310182101772806, -0.08412104610612851, -0.01768823865868867}, {0.21599452704334984, 0.034565881737666546, 0.06227463701473255, 0.021718356674803086, -0.2540992059785635, -0.494018403249423, -0.012144239564915057, 0.028518206714749087, -0.16661117670514075, -0.07958899955373971, 0.006716606300280974, 0.24781929437727154, -0.0789349834651409, 0.2527557298472126, 0.22600873999850843, 0.24153814191152542, -0.15238221104267374, 0.18963780402892336, 0.060293562217246355, -0.04075342366994371, 5.2750254107409723e-05, -0.04558440752746457, 0.3102353481637106, 0.004183882720940086, 0.12243072755811547, 0.16381384421215134, -0.2456936878307125, -0.20428351550141505, -0.1870783414447402, -0.0013390202927869412, 0.3413541251673451, 0.13898596868349797}, {0.04532946842357483, -0.11989333167787905, -0.11510389530363158, -0.17071847448666094, -0.2192984364037681, -0.08780220372740939, -0.03000033833055097, 0.04617146563432458, 0.23047273189584078, -0.17698735091254253, 0.15960265341011592, 0.1347142183673228, -0.22656223865039798, -0.04379693645327459, -0.01948892014774154, 0.3390098982314176, 0.26038196834108607, -0.09262357793568556, -0.19196826426285266, 0.13367658019889858, -0.0389487131814888, -0.17150172379855458, 0.08789115880485308, 0.21528798421162126, 0.3122851605372809, 0.03193576678182246, -0.29602420159163184, 0.15122563662114538, 0.21132244896535726, 0.05956161369688997, -0.12641831633388298, -0.023898981335892026}, {-0.0018549900250933711, -0.30841761387941946, -0.11931101607020328, -0.09851365272093336, -0.04126639329462396, -0.05335667409206914, -0.08045041265240631, -0.07531698941085647, 0.366287697272693, -0.06204332659918297, -0.1590270404469649, 0.3493616624550596, -0.18685214247183832, -0.08505115041288146, 0.14719118163981554, -0.10676572342275302, 0.28172914879365035, -0.23942503745937502, -0.24729334659269417, 0.1381251900303702, -0.0005090517165456505, 0.307956758543639, -0.04926763971361534, -0.04116494886138977, -0.2453995912484116, -0.11927617432997657, 0.33554016880615045, 0.31521512739627716, 0.09110377810370886, 0.28442551440235064, -0.15256753963443978, 0.2994820306676294}, {-0.18552085985945457, -0.030448672118619874, -0.04566290072281956, -0.057186658933929026, -0.17558209732659164, 0.1301705182284005, -0.014904428959368903, -0.060106586850978, 0.23498027883716252, 0.17023194230918198, -0.07943048848954741, 0.0807429055335873, -0.0024951506959999823, 0.005501825810966728, 0.2196598548044797, -0.2342075578651978, -0.3266312220881522, -0.021323971685697003, -0.11880004530792496, 0.017100802314521636, 8.62899915672038e-09, -0.14680182738524347, 0.23708763573306438, -0.17931516909138362, 0.054224693089308024, -0.11631413981875342, -0.0002267391410762023, 0.06918821074396704, 0.24344512655220527, 0.00022961382284346878, 0.04334981392914436, -0.011100573955152218}, {-0.241881943958038, -0.08641142925764131, -0.27994828486187734, -0.049700133025391884, 0.23541147845464502, -0.4122959192251607, 0.07642653748102068, -0.0745957548476416, 0.073039593312531, -0.17311897385728406, 0.014650540822105526, 0.17954613707902253, 0.3457018381858369, -0.06152643255918632, 0.0832536167363623, 0.15255415595867003, -0.0695151792621964, 0.2686994470645941, -0.26216242820357105, -0.15690302406675033, -1.989944159739009e-06, 0.2384268112786167, -0.018323115591200034, 0.13805652074200292, 0.010045007670827673, 0.07024421533282005, -0.3324974794221236, 0.2730128976286128, 0.1279474257071489, 0.14162874658964597, 0.10085199293197461, 0.10471528730152223}}, {{-0.30169150717052434, -0.3115278884231015, -0.2582585408614992, -0.045489311565443134}, {0.4191795616485859, 0.10700748401959935, 0.10787021195137686, -0.18980531569898393}, {-0.20099060334754187, 0.02363510942793779, -0.16107558724370138, -0.37730511410445755}, {0.0033540576577515244, 0.29719984912204456, 0.07458404070485679, -0.31083247360030103}, {-0.1408408651465714, 0.02381359998020129, 0.3233086919584141, 0.15674999294973274}, {0.5499523841060637, -0.17965522829423988, -0.09122325714446632, -0.5572767439589179}, {0.002361057995100606, 0.045405460791552214, 0.03663567818995681, 0.010620243032938986}, {-0.4053651749384924, -0.0840814833537319, 0.4393183458437282, -0.46294509549616086}, {0.02697585222373926, -0.486117621924583, 0.25472652206592045, -0.04995273608378439}, {0.4236569945423786, -0.29657305001330087, -0.09192740725369042, -0.3446217518859425}, {0.38355254837788844, -0.2652394354202226, -0.2562595260002138, -0.3349359477213139}, {-0.3831743960990716, -0.45389986043511193, 0.21821018306601628, 0.04698442845883499}, {0.18385196823560354, -0.14802384226511545, -0.3621000390297067, 0.26322369010996993}, {0.020070960936595012, -0.09327448108343855, -0.33644874724605056, -0.01936066394195045}, {0.29471065787608014, -0.022139221835130125, -0.28222764905669223, 0.4139557121741659}, {-0.3392334708631672, -0.27015735744426256, -0.3105213625169094, -0.028959469847858018}, {-0.18860446272104758, -0.283131292763725, 0.17096661269348895, -0.21480897676496732}, {-0.20990195798098318, 0.3740826515501552, 0.02934247851082567, -0.14876614317579429}, {0.38814202596266134, -0.10210431462702292, -0.39376609783989214, -0.30106195206583913}, {-0.3322471875066224, -0.7754408190079339, 0.6568564111111738, -0.5338266493709273}, {-0.12115136421365924, 0.11392974019344021, 0.08595655953442226, -0.020523619700285406}, {0.11225632601471973, 0.21578953374093673, 0.078377964822617, 0.2444928947192353}, {0.3784523413929714, 0.33440962915878353, 0.11897707346075458, 0.09789578416733266}, {-0.34745395643342325, -0.221917228604064, -0.0601437893433756, -0.06756066113823697}, {-0.20681215800040023, 0.4196779072501582, 0.015569925612596585, 0.27375071092857745}, {-0.0015743095477316535, -0.18870017015216412, 0.24377790397939159, -0.16965458727173113}, {-0.19651484664039723, -0.6991770262809728, 0.4924577159080791, -0.048183102976747154}, {0.06701630192737161, -0.2539974760936175, -0.16247597292229696, 0.40629527849822505}, {-0.3709387316635492, 0.022513800671841413, -0.21232939070834256, 0.013097946466306106}, {-0.11322792005845564, 0.008634349781088155, 0.3262907566659994, 0.23493885984207613}, {0.06375123121671215, 0.4430911154171553, 0.10493858521818, -0.22005073206021397}, {-0.3548004904144947, 0.1343517412618918, 0.22828634222688757, 0.046094984923429304}}};\n",
      "            double[][] bias = {{0.13339830747243805, -0.044779878190795035, 0.2562999979471128, -0.1944054971609164, 0.40295222233965666, 0.18215739671817993, 0.2375113977810496, 0.5012725060155155, 0.26602508997589147, -0.3175210042313719, 0.17027315439978968, 0.08880757649442513, 0.42678786986288275, -0.2157678966932607, -0.0281886383988633, -0.10263505044511442, -0.2880420264245715, 0.3176015009431075, 0.1018199797560431, 0.404218191345807, -0.21677567012853732, 0.33150655639365495, -0.29078403564723, -0.22854156070649395, 0.40526001608617845, 0.3362474095926491, 0.27795581810100406, 0.16457976020251647, 0.05903093932762925, -0.21050590328900742, 0.1268216268235701, -0.2936133663453412}, {-0.006094607231611321, -0.22262524909869508, 0.14982908404143028, 0.16944140514791328, 0.03978590444461457, -0.22213354162631338, 0.09234422241997567, 0.3203120616104526, -0.05250187452876851, 0.09857067457925466, -0.30877723469918333, 0.1820916679593901, -0.1863827543080164, -0.22609776559990227, 0.24992901427967523, 0.33651216071115664, 0.009948160801940758, 0.007437189406043011, 0.17238673051128703, 0.3491873533423273, -0.11390420701880752, -0.15623550052476418, -0.09707417313924013, -0.004928774559840383, -0.1344757684984585, -0.012323515640676578, 0.5978959413666703, 0.030313728283908693, -0.05514415722397253, -0.013005324181454373, 0.18450917777172166, -0.01529332372978267}, {0.1279508870870611, 0.13473438218442138, 0.20777745310631945, -0.26924572789893864}};\n",
      "\n",
      "            // Prediction:\n",
      "            MLPClassifier clf = new MLPClassifier(\"relu\", \"softmax\", layers, weights, bias);\n",
      "            int estimation = clf.predict(features);\n",
      "            System.out.println(estimation);\n",
      "\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "porter=sklearn_porter.Porter(model,language='java')\n",
    "java_code=porter.export()\n",
    "print(java_code)\n",
    "f=open('MLPClassifier.java','w')\n",
    "f.write(java_code)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([[12,-3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
